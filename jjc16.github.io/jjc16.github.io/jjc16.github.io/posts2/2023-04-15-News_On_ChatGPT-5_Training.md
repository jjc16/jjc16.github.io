According to the latest publicly available information, OpenAI is not currently training GPT-5 and "won't for some time". In public news releases, OpenAI seems to hint at an operational pause to implement greater AI safety and model transparency. An example article to read about this can be found [here](https://www.theverge.com/2023/4/14/23683084/openai-gpt-5-rumors-training-sam-altman).While it's likely that greater safety has *something* to do with the pause, I'm not convinced that this is the full story. I believe it's likely that OpenAI ran into difficulties with training GPT-5 (rumored to contain 10 trillion parameters) and decided to put their money towards trying to make GPT-4 better instead. 

If my theory is correct, then this likely indicates the point where the current architecture for Large Language Models (LLMs) starts to break down and resist further improvement. Based on previous experience I have trying to train similar models from scratch (specifically a transformer using the BERT architecture on aircraft maintenance jargon), they are probably hitting one or more of the following problems.

1. Disappearing gradients: Although OpenAI is remaining quiet about the implementation details for GPT-5, it's likely that the extra parameters came in the form of stacking additional transformer layers. My intuition is that they are eventually going to hit the same disappearing gradient constraints that motivated the ResNet architecture for Convolutional Neural Networks [link](https://arxiv.org/abs/1512.03385). I don't currently know if it's possible to sovle the disappearing gradient problem using skip connections with a transformer architecture. If it's not, then I could see disappearing gradients making the deepest layers of the network almost impossible to train.

2. Exponentially scaling training costs: The number of possible paths through the network (with is isomorphic to an electrical circuit) has a power law scaling with the number of weights. It's entirely possible that OpenAI threw a lot of money at trying to train the network and they weren't getting anywhere because of the sheer size. OpenAI [claims](https://arxiv.org/abs/2303.08774) to have a method to reduce the required number of training iterations that they used to train GPT-4, but I wonder if that training method scales with larger model architectures.

3. Increased brittleness for larger models: The current paradigm for LLMs is to use dropout layers to approximate a Bayesian response from the network [(link)](https://towardsdatascience.com/uncertainty-estimation-for-neural-network-dropout-as-bayesian-approximation-7d30fc7bc1f2) to try to estimate the robustness of the network to perturbations in the input data. As the size of the network grows, the path length of input signals through the network also grows. If we assume the weights are distributed locally in an approximately Gaussian distribution, this means that each signal will be exposed to more "random walk" kicks as it interacts with each network node. Ideally, the network would be trained such that the equilibria for the network are relatively stable so that each signal converges to the same or similar points under dropout perturbations. The mechanism behind this would be that the perturbations tend to cancel each other during signal transit. However, it's also possible that the network could have multiple unstable equilibria (i.e. "black swan events") and that the occurance of these equilibria could increase as a function of network size and complexity and could prove to be incredibly hard to root out during training in practice.

4. Inability to reuse **ANY** work from GPT-4: The problem with training LLMs using the "giant inscrutable matrics" [link](https://www.lesswrong.com/posts/GkC6YTu4DWp2zwf9k/giant-in-scrutable-matrices-maybe-the-best-of-all-possible) with tight, correlated coupling approach is that a) nobody knows exactly how they work and b) nobody knows which parts of which matrix actually do what, so it's impossible (with current) techniques to reuse the training work for more advanced models.

As an illustration of this point, imagine that you have a model with $N$ parameters, and you want to train a model with $2N$ parameters, possibly by stacking additional layers onto the back of your current tensor stack. With the current approaches, nobody (to my knowledge) knows how to leverage the current weights of the $N$ network to meaningfully reduce the work of training the $2N$ network versus starting over from a set of random initializations.  Note that I am making a distinction between adding more layers to the network versus swapping out classification heads (which is the approach used for a lot of the [HuggingFace](https://huggingface.co/) language models. 

There are probably other challenges that I've missed, and I may update the post later to include them, if I think of any. 
